{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Business Search        URL -- 'https://api.yelp.com/v3/businesses/search'\n",
    "#Business Match         URL -- 'https://api.yelp.com/v3/businesses/matches'\n",
    "#Phone Search           URL -- 'https://api.yelp.com/v3/businesses/search/phone'\n",
    "#Business Details       URL -- 'https://api.yelp.com/v3/businesses/{id}'\n",
    "#Business Reviews       URL -- 'https://api.yelp.com/v3/businesses/{id}/reviews'\n",
    "# article = 'https://open.lib.umn.edu/exploringbusiness/chapter/5-3-what-industries-are-small-businesses-in/#:~:text=About%2020%20percent%20of%20small,of%20the%20overall%20U.S.%20economy.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import requests\n",
    "import json\n",
    "from YelpAPI import get_my_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the API Key, define endpoint, define the header\n",
    "API_KEY = get_my_key()\n",
    "ENDPOINT = 'https://api.yelp.com/v3/businesses/search'\n",
    "HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "# Define the parameters\n",
    "PARAMETERS = {'limit': 50,\n",
    "              'offset': 0,\n",
    "              'term': 'construction',\n",
    "              'location': 'California'}\n",
    "# Make a request to Yelp API\n",
    "response = requests.get(url = ENDPOINT, params= PARAMETERS, headers= HEADERS)\n",
    "business_data = response.json()\n",
    "print(business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the JSON string to a dictionary\n",
    "g = open('construction.json')\n",
    "yelpdata = json.load(g)\n",
    "business_data = response.json()\n",
    "for biz in business_data['businesses']:\n",
    "    bizid = biz['id']\n",
    "    rating = biz['rating']\n",
    "    yelpdata[bizid] = rating\n",
    "with open('construction.json', 'w') as f:\n",
    "    json.dump(yelpdata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more data !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = get_my_key()\n",
    "ENDPOINT = 'https://api.yelp.com/v3/businesses/search'\n",
    "HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "    \n",
    "for i in range(20):\n",
    "    PARAMETERS = {'limit': 50,\n",
    "              'offset': 50 * i,\n",
    "              'term': 'construction',\n",
    "              'location': 'California'}\n",
    "    response = requests.get(url = ENDPOINT, params= PARAMETERS, headers= HEADERS)\n",
    "    g = open('restaurant.json')\n",
    "    yelpdata = json.load(g)\n",
    "    business_data = response.json()\n",
    "    for biz in business_data['businesses']:\n",
    "        bizid = biz['id']\n",
    "        rating = biz['rating']\n",
    "        yelpdata[bizid] = rating\n",
    "    with open('construction.json', 'w') as f:\n",
    "        json.dump(yelpdata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Reviews !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = get_my_key()\n",
    "business_id = '9M_FW_-Ipx93I36w-_ykBg'\n",
    "ENDPOINT = 'https://api.yelp.com/v3/businesses/{}/reviews'.format(business_id)\n",
    "HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "response = requests.get(url = ENDPOINT, headers= HEADERS)\n",
    "business_data = response.json()\n",
    "attitude = []\n",
    "reviews = []\n",
    "for review in business_data['reviews']:\n",
    "    reviews.append(review['text'])\n",
    "    rating = review['rating']\n",
    "    if rating > 4:\n",
    "        temp = 1\n",
    "    else:\n",
    "        temp = -1\n",
    "    attitude.append(temp)\n",
    "with open('const_review.json', 'w') as f:\n",
    "    json.dump(reviews, f, indent=2)\n",
    "with open('const_sentiment.json', 'w') as f:\n",
    "    json.dump(attitude, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Up. Massive Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = get_my_key()\n",
    "g = open('construction.json')\n",
    "yelpdata = json.load(g)\n",
    "id_list = list(yelpdata.keys())\n",
    "for element in id_list:\n",
    "    business_id = element\n",
    "    ENDPOINT = 'https://api.yelp.com/v3/businesses/{}/reviews'.format(business_id)\n",
    "    HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "    response = requests.get(url = ENDPOINT, headers= HEADERS)\n",
    "    business_data = response.json()\n",
    "    g = open('const_sentiment.json')\n",
    "    attitude = json.load(g)\n",
    "    g = open('const_review.json')\n",
    "    reviews = json.load(g)\n",
    "    if 'error' in business_data:\n",
    "        continue\n",
    "    for review in business_data['reviews']:\n",
    "        reviews.append(review['text'])\n",
    "        rating = review['rating']\n",
    "        if rating > 4:\n",
    "            temp = 1\n",
    "        else:\n",
    "            temp = -1\n",
    "        attitude.append(temp)\n",
    "    with open('const_review.json', 'w') as f:\n",
    "        json.dump(reviews, f, indent=2)\n",
    "    with open('const_sentiment.json', 'w') as f:\n",
    "        json.dump(attitude, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data has been gathered. New User please run from here\n",
    "### This section is for functions that we will initially use\n",
    "### We will use the scikit learn library eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "from YelpAPI import get_my_key\n",
    "\n",
    "def get_order(n_samples, t):\n",
    "    random.seed(t)\n",
    "    indices = list(range(n_samples))\n",
    "    random.shuffle(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def perceptron_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        current_theta,\n",
    "        current_theta_0):\n",
    "    \"\"\"\n",
    "    Properly updates the classification parameter, theta and theta_0, on a\n",
    "    single step of the perceptron algorithm.\n",
    "\n",
    "    Args:\n",
    "        feature_vector - A numpy array describing a single data point.\n",
    "        label - The correct classification of the feature vector.\n",
    "        current_theta - The current theta being used by the perceptron\n",
    "            algorithm before this update.\n",
    "        current_theta_0 - The current theta_0 being used by the perceptron\n",
    "            algorithm before this update.\n",
    "\n",
    "    Returns: A tuple where the first element is a numpy array with the value of\n",
    "    theta after the current update has completed and the second element is a\n",
    "    real valued number with the value of theta_0 after the current updated has\n",
    "    completed.\n",
    "    \"\"\"\n",
    "    if label * (np.dot(current_theta, feature_vector) + current_theta_0) <= 1e-7:\n",
    "        current_theta += label * feature_vector\n",
    "        current_theta_0 += label\n",
    "    return (current_theta, current_theta_0)\n",
    "\n",
    "def perceptron(feature_matrix, labels, T):\n",
    "    \"\"\"\n",
    "    Runs the full perceptron algorithm on a given set of data. Runs T\n",
    "    iterations through the data set, there is no need to worry about\n",
    "    stopping early.\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.\n",
    "    Do not copy paste code from previous parts.\n",
    "\n",
    "    NOTE: Iterate the data matrix by the orders returned by get_order(feature_matrix.shape[0])\n",
    "\n",
    "    Args:\n",
    "        feature_matrix -  A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        labels - A numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        T - An integer indicating how many times the perceptron algorithm\n",
    "            should iterate through the feature matrix.\n",
    "\n",
    "    Returns: A tuple where the first element is a numpy array with the value of\n",
    "    theta, the linear classification parameter, after T iterations through the\n",
    "    feature matrix and the second element is a real number with the value of\n",
    "    theta_0, the offset classification parameter, after T iterations through\n",
    "    the feature matrix.\n",
    "    \"\"\"\n",
    "    (nsamples, nfeatures) = feature_matrix.shape\n",
    "    theta = np.zeros(nfeatures)\n",
    "    theta_0 = 0.0\n",
    "    for t in range(T):\n",
    "        order = get_order(nsamples, t + 1)\n",
    "        for i in order:\n",
    "            theta, theta_0 = perceptron_single_step_update(feature_matrix[i], labels[i], theta, theta_0)\n",
    "    return (theta, theta_0)\n",
    "\n",
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \"\"\"\n",
    "    A classification function that uses theta and theta_0 to classify a set of\n",
    "    data points.\n",
    "\n",
    "    Args:\n",
    "        feature_matrix - A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        theta - A numpy array describing the linear classifier.\n",
    "        theta_0 - A real valued number representing the offset parameter.\n",
    "\n",
    "    Returns: A numpy array of 1s and -1s where the kth element of the array is\n",
    "    the predicted classification of the kth row of the feature matrix using the\n",
    "    given theta and theta_0. If a prediction is GREATER THAN zero, it should\n",
    "    be considered a positive classification.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    result = np.zeros(feature_matrix.shape[0])\n",
    "    for i in range(feature_matrix.shape[0]):\n",
    "        value = np.dot(feature_matrix[i], theta) + theta_0\n",
    "        if value > 0:\n",
    "            result[i] = 1\n",
    "        else:\n",
    "            result[i] = -1\n",
    "    return result\n",
    "\n",
    "def bag_of_words(texts):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Returns a dictionary of unique unigrams occurring over the input\n",
    "\n",
    "    Feel free to change this code as guided by Problem 9\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    dictionary = {} # maps word to unique index\n",
    "    for text in texts:\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "def extract_bow_feature_vectors(reviews, dictionary):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Inputs the dictionary of words as given by bag_of_words\n",
    "    Returns the bag-of-words feature matrix representation of the data.\n",
    "    The returned matrix is of shape (n, m), where n is the number of reviews\n",
    "    and m the total number of entries in the dictionary.\n",
    "\n",
    "    Feel free to change this code as guided by Problem 9\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    num_reviews = len(reviews)\n",
    "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
    "\n",
    "    for i, text in enumerate(reviews):\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word in dictionary:\n",
    "                feature_matrix[i, dictionary[word]] += 1\n",
    "    return feature_matrix\n",
    "\n",
    "def most_explanatory_word(theta, wordlist):\n",
    "    \"\"\"Returns the word associated with the bag-of-words feature having largest weight.\"\"\"\n",
    "    return [word for (theta_i, word) in sorted(zip(theta, wordlist))[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please Download nltk natural language processing library for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/qinjianxyz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qinjianxyz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_words(text):\n",
    "    '''\n",
    "    Helper function for bag_of_words()\n",
    "    Inputs a text string\n",
    "    '''\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to input our data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Explanatory Word Features\n",
      "['civita', 'rachel', 'islands', 'opportune', 'feech', 'affairs', 'smothered', 'frostie', 'strictly', 'signage', 'etiquette', 'arts', 'square', 'logistics', 'indianpakistani', 'everywhere', 'trigger', 'prompt', 'post', 'maskless', 'fixings', 'batch', 'aound', 'wishing', 'term', 'suddenly', 'snow', 'overwhelming', 'listed', 'juicy']\n"
     ]
    }
   ],
   "source": [
    "g = open('yelp_sentiment.json')\n",
    "train_labels = json.load(g)\n",
    "g = open('yelp_review.json')\n",
    "train_texts = json.load(g)\n",
    "dictionary = bag_of_words(train_texts)\n",
    "train_features = extract_bow_feature_vectors(train_texts, dictionary)\n",
    "theta, theta_0 = perceptron(train_features, train_labels, 1000)\n",
    "wordlist = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "sorted_word_features = most_explanatory_word(theta, wordlist)\n",
    "print(\"Most Explanatory Word Features\")\n",
    "print(sorted_word_features[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most explanatory words:** ['civita', 'rachel', 'islands', 'opportune', 'feech', 'affairs', 'smothered', 'frostie', 'strictly', 'signage', 'etiquette', 'arts', 'square', 'logistics', 'indianpakistani', 'everywhere', 'trigger', 'prompt', 'post', 'maskless', 'fixings', 'batch', 'aound', 'wishing', 'term', 'suddenly', 'snow', 'overwhelming', 'listed', 'juicy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant Business Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Explanatory Word Features\n",
      "['gem', 'wonderful', 'update', 'love', 'worked', 'tough', 'tacos', 'phase', 'party', 'nicely', 'next', 'mammoth', 'indoor', 'best', 'absolutely', 'visiting', 'usually', 'urban', 'tuna', 'site', 'short', 'servings', 'right', 'recommendation', 'recently', 'reasonable', 'quarantine', 'pokiland', 'patio', 'pandemic']\n"
     ]
    }
   ],
   "source": [
    "g = open('rest_sentiment.json')\n",
    "train_labels = json.load(g)\n",
    "g = open('rest_review.json')\n",
    "train_texts = json.load(g)\n",
    "dictionary = bag_of_words(train_texts)\n",
    "train_features = extract_bow_feature_vectors(train_texts, dictionary)\n",
    "theta, theta_0 = perceptron(train_features, train_labels, 1000)\n",
    "wordlist = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "sorted_word_features = most_explanatory_word(theta, wordlist)\n",
    "print(\"Most Explanatory Word Features\")\n",
    "print(sorted_word_features[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most explanatory words:** ['gem', 'wonderful', 'update', 'love', 'worked', 'tough', 'tacos', 'phase', 'party', 'nicely', 'next', 'mammoth', 'indoor', 'best', 'absolutely', 'visiting', 'usually', 'urban', 'tuna', 'site', 'short', 'servings', 'right', 'recommendation', 'recently', 'reasonable', 'quarantine', 'pokiland', 'patio', 'pandemic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Business Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Explanatory Word Features\n",
      "['wonderful', 'update', 'seating', 'quarantine', 'party', 'indoor', 'gem', 'dash', 'worked', 'wanted', 'visiting', 'usually', 'tough', 'support', 'short', 'phase', 'park', 'pandemic', 'ordering', 'love', 'even', 'enjoyed', 'easily', 'decided', 'clovis', 'awesome', 'absolutely', 'view', 'updated', 'tacos']\n"
     ]
    }
   ],
   "source": [
    "g = open('const_sentiment.json')\n",
    "train_labels = json.load(g)\n",
    "g = open('const_review.json')\n",
    "train_texts = json.load(g)\n",
    "dictionary = bag_of_words(train_texts)\n",
    "train_features = extract_bow_feature_vectors(train_texts, dictionary)\n",
    "theta, theta_0 = perceptron(train_features, train_labels, 1000)\n",
    "wordlist = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "sorted_word_features = most_explanatory_word(theta, wordlist)\n",
    "print(\"Most Explanatory Word Features\")\n",
    "print(sorted_word_features[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most explanatory words:** ['wonderful', 'update', 'seating', 'quarantine', 'party', 'indoor', 'gem', 'dash', 'worked', 'wanted', 'visiting', 'usually', 'tough', 'support', 'short', 'phase', 'park', 'pandemic', 'ordering', 'love', 'even', 'enjoyed', 'easily', 'decided', 'clovis', 'awesome', 'absolutely', 'view', 'updated', 'tacos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction with SciKit Learn Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Balboa Park is definitely one place to visit i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Even though it was gloomy and breezy out! It d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World class gem of a park.  I have been here b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drove by and saw the crazy long line at Phil's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Awesome service we order our food and when It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Sentiment\n",
       "0  Balboa Park is definitely one place to visit i...          1\n",
       "1  Even though it was gloomy and breezy out! It d...          1\n",
       "2  World class gem of a park.  I have been here b...          1\n",
       "3  Drove by and saw the crazy long line at Phil's...          1\n",
       "4  Awesome service we order our food and when It ...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = open('yelp_review.json')\n",
    "text = json.load(g)\n",
    "g = open('yelp_sentiment.json')\n",
    "sentiment = json.load(g)\n",
    "d = {'Reviews': text, 'Sentiment': sentiment}\n",
    "df = pd.DataFrame(d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_clean(text):\n",
    "    '''\n",
    "    Helper function for bag_of_words()\n",
    "    Inputs a text string\n",
    "    '''\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    sentence = \"\"\n",
    "    for word in words:\n",
    "        sentence = sentence + word + \" \"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balboa park definitely one place visit ever sa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even though gloomy breezy nt stop us visiting ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>world class gem park time walked around park s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drove saw crazy long line phil great sign deci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome service order food came thought order ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Sentiment\n",
       "0  balboa park definitely one place visit ever sa...          1\n",
       "1  even though gloomy breezy nt stop us visiting ...          1\n",
       "2  world class gem park time walked around park s...          1\n",
       "3  drove saw crazy long line phil great sign deci...          1\n",
       "4  awesome service order food came thought order ...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Reviews'] = df['Reviews'].apply(lambda x: get_clean(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "X = df['Reviews']\n",
    "y = df['Sentiment']\n",
    "\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.48      0.54       608\n",
      "           1       0.82      0.89      0.85      1619\n",
      "\n",
      "    accuracy                           0.77      2227\n",
      "   macro avg       0.72      0.68      0.69      2227\n",
      "weighted avg       0.76      0.77      0.77      2227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'The restaurant has a very nice environment'\n",
    "#x = 'The waiter here is rude and slow'\n",
    "#x = 'I like the vegan options a lot'\n",
    "#x = 'The store requires masks and I hate that'\n",
    "x = get_clean(x)\n",
    "vec = tfidf.transform([x])\n",
    "clf.predict(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location visualization with folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import requests\n",
    "import json\n",
    "from YelpAPI import get_my_key\n",
    "\n",
    "\n",
    "m = folium.Map(location=[32.7157, -117.1611], zoom_start=12, tiles=\"Stamen Terrain\")\n",
    "\n",
    "API_KEY = get_my_key()\n",
    "g = open('yelp.json')\n",
    "yelpdata = json.load(g)\n",
    "id_list = list(yelpdata.keys())\n",
    "for element in id_list:\n",
    "    business_id = element\n",
    "    ENDPOINT = 'https://api.yelp.com/v3/businesses/{}'.format(business_id)\n",
    "    HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "    response = requests.get(url = ENDPOINT, headers= HEADERS)\n",
    "    biz = response.json()\n",
    "    if 'error' in biz:\n",
    "        continue\n",
    "    if biz['coordinates']['latitude'] == None:\n",
    "        continue\n",
    "    if biz['coordinates']['longitude'] == None:\n",
    "        continue\n",
    "    if biz['name'] == None:\n",
    "        continue\n",
    "    name = biz['name']\n",
    "    la = biz['coordinates']['latitude']\n",
    "    lon = biz['coordinates']['longitude']\n",
    "    rating = biz['rating']\n",
    "    if rating > 4.7:\n",
    "        col = 'darkgreen'\n",
    "    elif rating > 4.2:\n",
    "        col = 'green'\n",
    "    elif rating > 3.7:\n",
    "        col = 'orange'\n",
    "    elif rating > 3.2:\n",
    "        col = 'lightred'\n",
    "    else:\n",
    "        col = 'red'\n",
    "    folium.Marker(\n",
    "    location=[la, lon],\n",
    "    popup=name,\n",
    "    icon=folium.Icon(color=col),\n",
    "    ).add_to(m)\n",
    "    \n",
    "m.save(\"update.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'code': 'ACCESS_LIMIT_REACHED', 'description': \"You've reached the access limit for this client. See instructions for requesting a higher access limit at https://www.yelp.com/developers/documentation/v3/rate_limiting\"}}\n"
     ]
    }
   ],
   "source": [
    "print(biz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restaurant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import requests\n",
    "import json\n",
    "from YelpAPI import get_my_key\n",
    "\n",
    "\n",
    "m = folium.Map(location=[32.7157, -117.1611], zoom_start=12, tiles=\"Stamen Terrain\")\n",
    "\n",
    "API_KEY = get_my_key()\n",
    "g = open('restaurant.json')\n",
    "yelpdata = json.load(g)\n",
    "id_list = list(yelpdata.keys())\n",
    "for element in id_list:\n",
    "    business_id = element\n",
    "    ENDPOINT = 'https://api.yelp.com/v3/businesses/{}'.format(business_id)\n",
    "    HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "    response = requests.get(url = ENDPOINT, headers= HEADERS)\n",
    "    biz = response.json()\n",
    "    if 'error' in biz:\n",
    "        continue\n",
    "    if biz['coordinates']['latitude'] == None:\n",
    "        continue\n",
    "    if biz['coordinates']['longitude'] == None:\n",
    "        continue\n",
    "    if biz['name'] == None:\n",
    "        continue\n",
    "    name = biz['name']\n",
    "    la = biz['coordinates']['latitude']\n",
    "    lon = biz['coordinates']['longitude']\n",
    "    rating = biz['rating']\n",
    "    if rating > 4.7:\n",
    "        col = 'darkgreen'\n",
    "    elif rating > 4.2:\n",
    "        col = 'green'\n",
    "    elif rating > 3.7:\n",
    "        col = 'orange'\n",
    "    elif rating > 3.2:\n",
    "        col = 'lightred'\n",
    "    else:\n",
    "        col = 'red'\n",
    "    folium.Marker(\n",
    "    location=[la, lon],\n",
    "    popup=name,\n",
    "    icon=folium.Icon(color=col),\n",
    "    ).add_to(m)\n",
    "    \n",
    "m.save(\"restaurant.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
