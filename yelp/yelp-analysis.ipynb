{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Business Search        URL -- 'https://api.yelp.com/v3/businesses/search'\n",
    "#Business Match         URL -- 'https://api.yelp.com/v3/businesses/matches'\n",
    "#Phone Search           URL -- 'https://api.yelp.com/v3/businesses/search/phone'\n",
    "#Business Details       URL -- 'https://api.yelp.com/v3/businesses/{id}'\n",
    "#Business Reviews       URL -- 'https://api.yelp.com/v3/businesses/{id}/reviews'\n",
    "# article = 'https://open.lib.umn.edu/exploringbusiness/chapter/5-3-what-industries-are-small-businesses-in/#:~:text=About%2020%20percent%20of%20small,of%20the%20overall%20U.S.%20economy.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import requests\n",
    "import json\n",
    "from YelpAPI import get_my_key\n",
    "\n",
    "# Define the API Key, define endpoint, define the header\n",
    "API_KEY = get_my_key()\n",
    "ENDPOINT = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "\n",
    "# Define the parameters\n",
    "PARAMETERS = {'limit': 50,\n",
    "              'offset': 50,\n",
    "              'location': 'San Diego'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to Yelp API\n",
    "\n",
    "response = requests.get(url = ENDPOINT, params= PARAMETERS, headers= HEADERS)\n",
    "\n",
    "# convert the JSON string to a dictionary\n",
    "g = open('yelpdata.json')\n",
    "yelpdata = json.load(g)\n",
    "business_data = response.json()\n",
    "for biz in business_data['businesses']:\n",
    "    bizid = biz['id']\n",
    "    rating = biz['rating']\n",
    "    yelpdata[bizid] = rating\n",
    "with open('yelpdata.json', 'w') as f:\n",
    "    json.dump(yelpdata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more data !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = get_my_key()\n",
    "ENDPOINT = 'https://api.yelp.com/v3/businesses/search'\n",
    "HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "    \n",
    "for i in range(100):\n",
    "    PARAMETERS = {'limit': 50,\n",
    "                  'offset': 50 * i,\n",
    "                  'location': 'San Diego'}\n",
    "    response = requests.get(url = ENDPOINT, params= PARAMETERS, headers= HEADERS)\n",
    "    g = open('yelpdata.json')\n",
    "    yelpdata = json.load(g)\n",
    "    business_data = response.json()\n",
    "    for biz in business_data['businesses']:\n",
    "        bizid = biz['id']\n",
    "        rating = biz['rating']\n",
    "        yelpdata[bizid] = rating\n",
    "    with open('yelpdata.json', 'w') as f:\n",
    "        json.dump(yelpdata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Reviews !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = get_my_key()\n",
    "business_id = '9M_FW_-Ipx93I36w-_ykBg'\n",
    "ENDPOINT = 'https://api.yelp.com/v3/businesses/{}/reviews'.format(business_id)\n",
    "HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "response = requests.get(url = ENDPOINT, headers= HEADERS)\n",
    "business_data = response.json()\n",
    "attitude = []\n",
    "reviews = []\n",
    "for review in business_data['reviews']:\n",
    "    reviews.append(review['text'])\n",
    "    rating = review['rating']\n",
    "    if rating > 4:\n",
    "        temp = 1\n",
    "    else:\n",
    "        temp = -1\n",
    "    attitude.append(temp)\n",
    "with open('review_text.json', 'w') as f:\n",
    "    json.dump(reviews, f, indent=2)\n",
    "with open('sentiment.json', 'w') as f:\n",
    "    json.dump(attitude, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Up. Massive Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = get_my_key()\n",
    "g = open('yelpdata.json')\n",
    "yelpdata = json.load(g)\n",
    "id_list = list(yelpdata.keys())\n",
    "for element in id_list:\n",
    "    business_id = element\n",
    "    ENDPOINT = 'https://api.yelp.com/v3/businesses/{}/reviews'.format(business_id)\n",
    "    HEADERS = {'Authorization': 'bearer %s' % API_KEY}\n",
    "    response = requests.get(url = ENDPOINT, headers= HEADERS)\n",
    "    business_data = response.json()\n",
    "    g = open('sentiment.json')\n",
    "    attitude = json.load(g)\n",
    "    g = open('review_text.json')\n",
    "    reviews = json.load(g)\n",
    "    for review in business_data['reviews']:\n",
    "        reviews.append(review['text'])\n",
    "        rating = review['rating']\n",
    "        if rating > 4:\n",
    "            temp = 1\n",
    "        else:\n",
    "            temp = -1\n",
    "        attitude.append(temp)\n",
    "    with open('review_text.json', 'w') as f:\n",
    "        json.dump(reviews, f, indent=2)\n",
    "    with open('sentiment.json', 'w') as f:\n",
    "        json.dump(attitude, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data has been gathered. New User please run from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section is for functions that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "from YelpAPI import get_my_key\n",
    "\n",
    "def get_order(n_samples, t):\n",
    "    random.seed(t)\n",
    "    indices = list(range(n_samples))\n",
    "    random.shuffle(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def perceptron_single_step_update(\n",
    "        feature_vector,\n",
    "        label,\n",
    "        current_theta,\n",
    "        current_theta_0):\n",
    "    \"\"\"\n",
    "    Properly updates the classification parameter, theta and theta_0, on a\n",
    "    single step of the perceptron algorithm.\n",
    "\n",
    "    Args:\n",
    "        feature_vector - A numpy array describing a single data point.\n",
    "        label - The correct classification of the feature vector.\n",
    "        current_theta - The current theta being used by the perceptron\n",
    "            algorithm before this update.\n",
    "        current_theta_0 - The current theta_0 being used by the perceptron\n",
    "            algorithm before this update.\n",
    "\n",
    "    Returns: A tuple where the first element is a numpy array with the value of\n",
    "    theta after the current update has completed and the second element is a\n",
    "    real valued number with the value of theta_0 after the current updated has\n",
    "    completed.\n",
    "    \"\"\"\n",
    "    if label * (np.dot(current_theta, feature_vector) + current_theta_0) <= 1e-7:\n",
    "        current_theta += label * feature_vector\n",
    "        current_theta_0 += label\n",
    "    return (current_theta, current_theta_0)\n",
    "\n",
    "def perceptron(feature_matrix, labels, T):\n",
    "    \"\"\"\n",
    "    Runs the full perceptron algorithm on a given set of data. Runs T\n",
    "    iterations through the data set, there is no need to worry about\n",
    "    stopping early.\n",
    "\n",
    "    NOTE: Please use the previously implemented functions when applicable.\n",
    "    Do not copy paste code from previous parts.\n",
    "\n",
    "    NOTE: Iterate the data matrix by the orders returned by get_order(feature_matrix.shape[0])\n",
    "\n",
    "    Args:\n",
    "        feature_matrix -  A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        labels - A numpy array where the kth element of the array is the\n",
    "            correct classification of the kth row of the feature matrix.\n",
    "        T - An integer indicating how many times the perceptron algorithm\n",
    "            should iterate through the feature matrix.\n",
    "\n",
    "    Returns: A tuple where the first element is a numpy array with the value of\n",
    "    theta, the linear classification parameter, after T iterations through the\n",
    "    feature matrix and the second element is a real number with the value of\n",
    "    theta_0, the offset classification parameter, after T iterations through\n",
    "    the feature matrix.\n",
    "    \"\"\"\n",
    "    (nsamples, nfeatures) = feature_matrix.shape\n",
    "    theta = np.zeros(nfeatures)\n",
    "    theta_0 = 0.0\n",
    "    for t in range(T):\n",
    "        order = get_order(nsamples, t + 1)\n",
    "        for i in order:\n",
    "            theta, theta_0 = perceptron_single_step_update(feature_matrix[i], labels[i], theta, theta_0)\n",
    "    return (theta, theta_0)\n",
    "\n",
    "def classify(feature_matrix, theta, theta_0):\n",
    "    \"\"\"\n",
    "    A classification function that uses theta and theta_0 to classify a set of\n",
    "    data points.\n",
    "\n",
    "    Args:\n",
    "        feature_matrix - A numpy matrix describing the given data. Each row\n",
    "            represents a single data point.\n",
    "        theta - A numpy array describing the linear classifier.\n",
    "        theta_0 - A real valued number representing the offset parameter.\n",
    "\n",
    "    Returns: A numpy array of 1s and -1s where the kth element of the array is\n",
    "    the predicted classification of the kth row of the feature matrix using the\n",
    "    given theta and theta_0. If a prediction is GREATER THAN zero, it should\n",
    "    be considered a positive classification.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    result = np.zeros(feature_matrix.shape[0])\n",
    "    for i in range(feature_matrix.shape[0]):\n",
    "        value = np.dot(feature_matrix[i], theta) + theta_0\n",
    "        if value > 0:\n",
    "            result[i] = 1\n",
    "        else:\n",
    "            result[i] = -1\n",
    "    return result\n",
    "\n",
    "def bag_of_words(texts):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Returns a dictionary of unique unigrams occurring over the input\n",
    "\n",
    "    Feel free to change this code as guided by Problem 9\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    dictionary = {} # maps word to unique index\n",
    "    for text in texts:\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "def extract_bow_feature_vectors(reviews, dictionary):\n",
    "    \"\"\"\n",
    "    Inputs a list of string reviews\n",
    "    Inputs the dictionary of words as given by bag_of_words\n",
    "    Returns the bag-of-words feature matrix representation of the data.\n",
    "    The returned matrix is of shape (n, m), where n is the number of reviews\n",
    "    and m the total number of entries in the dictionary.\n",
    "\n",
    "    Feel free to change this code as guided by Problem 9\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    num_reviews = len(reviews)\n",
    "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
    "\n",
    "    for i, text in enumerate(reviews):\n",
    "        word_list = extract_words(text)\n",
    "        for word in word_list:\n",
    "            if word in dictionary:\n",
    "                feature_matrix[i, dictionary[word]] += 1\n",
    "    return feature_matrix\n",
    "\n",
    "def most_explanatory_word(theta, wordlist):\n",
    "    \"\"\"Returns the word associated with the bag-of-words feature having largest weight.\"\"\"\n",
    "    return [word for (theta_i, word) in sorted(zip(theta, wordlist))[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please Download nltk natural language processing library for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/qinjianxyz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qinjianxyz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_words(text):\n",
    "    '''\n",
    "    Helper function for bag_of_words()\n",
    "    Inputs a text string\n",
    "    '''\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to input our data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open('sentiment.json')\n",
    "train_labels = json.load(g)\n",
    "g = open('review_text.json')\n",
    "train_texts = json.load(g)\n",
    "dictionary = bag_of_words(train_texts)\n",
    "train_features = extract_bow_feature_vectors(train_texts, dictionary)\n",
    "theta, theta_0 = perceptron(train_features, train_labels, 1000)\n",
    "wordlist = [word for (idx, word) in sorted(zip(dictionary.values(), dictionary.keys()))]\n",
    "sorted_word_features = most_explanatory_word(theta, wordlist)\n",
    "print(\"Most Explanatory Word Features\")\n",
    "print(sorted_word_features[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Feature += 1) Most explanatory words:** ['twice', 'vegan', 'yet', 'thank', 'packed', 'omg', 'limited', 'la', 'keep', 'year', 'worth', 'weird', 'want', 'usually', 'tofu', 'thought', 'street', 'stephen', 'spotty', 'sandwiches', 'quickly', 'pacific', 'owner', 'ocean', 'ob', 'miss', 'longer', 'line', 'instead', 'ingredients', 'handling', 'excited', 'end', 'earlier', 'dumplings', 'curry', 'cakes', 'cafe', 'works', 'win', 'warm', 'walked', 'togo', 'still', 'reviewed', 'restaurants', 'reservation', 'quality', 'prices', 'precautions', 'polite', 'plate', 'pineapple', 'phenomenal', 'perfect', 'outstanding', 'opened', 'number', 'notice', 'nom', 'months', 'monday', 'mocha', 'left', 'kids', 'incredible', 'impeccably', 'hillcrest', 'grubhub', 'garlic', 'finally', 'favorite', 'exceptional', 'every', 'ever', 'enjoyable', 'enjoy', 'enchilada', 'dish', 'customers', 'current', 'creamy', 'corner', 'closer', 'chose', 'business', 'bomb', 'boats', 'bite', 'best', 'arrived', 'app', 'added', 'write', 'wore', 'wonderful', 'whenever', 'went', 'way', 'watch'] <br />\n",
    "**(Feature = 1) Most explanatory words:** ['yet', 'la', 'vegan', 'usually', 'omg', 'cafe', 'worth', 'twice', 'tofu', 'reviewed', 'real', 'dumplings', 'year', 'write', 'wore', 'want', 'walked', 'tell', 'surf', 'street', 'stephen', 'return', 'restaurants', 'reservation', 'pieces', 'owner', 'outstanding', 'opening', 'opened', 'obsession', 'number', 'monday', 'miss', 'longer', 'line', 'limited', 'left', 'kids', 'incredible', 'immediately', 'hillcrest', 'excited', 'earlier', 'corn', 'completely', 'ca', 'business', 'bite', 'bartender', 'added', 'wonderful', 'went', 'welcoming', 'weird', 'watch', 'valentine', 'upstairs', 'totally', 'tonya', 'thank', 'stunning', 'steak', 'stand', 'spotty', 'sports', 'size', 'sister', 'sandwiches', 'rider', 'reserve', 'quickly', 'quarantine', 'quality', 'prices', 'prefer', 'precautions', 'post', 'polite', 'plate', 'pineapple', 'pie', 'perfect', 'pasta', 'partner', 'packed', 'opportunity', 'ocean', 'ob', 'notice', 'noon', 'nom', 'morning', 'menu', 'lovely', 'lounges', 'lots', 'later', 'knowledgeable', 'knew', 'keeping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
